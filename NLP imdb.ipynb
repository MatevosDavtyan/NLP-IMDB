{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8b0c253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split , GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler , MaxAbsScaler , MinMaxScaler \n",
    "from sklearn.metrics import accuracy_score , classification_report , confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression , LogisticRegressionCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import warnings\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "encoder=OrdinalEncoder()\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0491a6ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "0      One of the other reviewers has mentioned that ...  positive\n",
       "1      A wonderful little production. <br /><br />The...  positive\n",
       "2      I thought this was a wonderful way to spend ti...  positive\n",
       "3      Basically there's a family where a little boy ...  negative\n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "...                                                  ...       ...\n",
       "49995  I thought this movie did a down right good job...  positive\n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
       "49997  I am a Catholic taught in parochial elementary...  negative\n",
       "49998  I'm going to have to disagree with the previou...  negative\n",
       "49999  No one expects the Star Trek movies to be high...  negative\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('IMDB Dataset.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dfe9e58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "df['review'] = df['review'].str.replace(\"<br /><br />\", \" \")\n",
    "df['review'] = df['review'].str.lower()\n",
    "def remove_numeric_symbols(text):\n",
    "    return ''.join([char for char in text if not char.isdigit()])\n",
    "df['review'] = df['review'].apply(remove_numeric_symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0afb039a",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_dict = {'positive':1,\n",
    "           'negative':0}\n",
    "df['sentiment'] = df.sentiment.map(map_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dbe445ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<50000x20477 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 4131429 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data = df['review'].values.flatten()\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# CountVectorizer(ngram_range=(2,2))  Bi-grams are two-word combinations max2 min 2\n",
    "vect = CountVectorizer(min_df=15,stop_words=\"english\").fit(text_data) #min_df minimum qanaky for texterum pti lini\n",
    "X = vect.transform(text_data)\n",
    "y=df['sentiment']\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e9aa4e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(X,\n",
    "                                                 y,\n",
    "                                                 test_size=0.2,\n",
    "                                                 random_state=42,\n",
    "                                                stratify=df.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4707dff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a=x_train.toarray()\n",
    "# b=x_test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b78278",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "scores = cross_val_score(LogisticRegression(C=0.1), x_train, y_train, cv=5)\n",
    "print(\"Средняя правильность перекр проверки: {:.2f}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6005194c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# param_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}\n",
    "# grid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
    "# grid.fit(x_train, y_train)\n",
    "# print(\"Наилучшее значение перекрестной проверки: {:.2f}\".format(grid.best_score_))\n",
    "# print(\"Наилучшие параметры: \", grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffbce101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=LogisticRegression().fit(x_train,y_train)\n",
    "v=model.predict(x_test)\n",
    "accuracy_score(v,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7575233a",
   "metadata": {},
   "source": [
    "#### TF-IDF  Этот метод учитывает не только количество вхождений слова в документе (частоту), но и важность слова в контексте всей коллекции документов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2f57ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = df['review'].values.flatten()\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# vectt = TfidfVectorizer(min_df=15,stop_words=\"english\").fit(text_data) #min_df minimum qanaky for texterum pti lini\n",
    "# ENGLISH_STOP_WORDS.union(['film', 'movie', 'cinema', 'theatre']) avelacnenq taza barer\n",
    "vectt = TfidfVectorizer(min_df=5,norm=None).fit(text_data) #min_df minimum qanaky for texterum pti lini\n",
    "X = vectt.transform(text_data)\n",
    "y=df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55a3974e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(X,\n",
    "                                                 y,\n",
    "                                                 test_size=0.2,\n",
    "                                                 random_state=42,\n",
    "                                                stratify=df.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "385b4b77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8835"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=LogisticRegression().fit(x_train,y_train)\n",
    "v=model.predict(x_test)\n",
    "accuracy_score(v,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e78682c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['____', '_is_', '_the', ..., 'élan', 'émigré', 'über'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = vectt.get_feature_names_out()\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b6ab25f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9.87388814,  9.74035674, 19.24514741, ...,  9.87388814,\n",
       "        9.74035674,  9.33489163])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_value = X.max(axis=0).toarray().ravel()\n",
    "#The ravel() method is used to flatten the resulting NumPy array into a 1D array. \n",
    "max_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9af5338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 100 Most Important Words:\n",
      "['undoubtedly', 'avoided', 'awhile', 'satisfy', 'suffice', 'assumed', 'lasting', 'shoulders', 'overlong', 'sums', 'demanding', 'reluctant', 'rapidly', 'wrenching', 'sticking', 'vain', 'earn', 'vastly', 'lush', 'blatantly', 'burst', 'contribution', 'suggestion', 'energetic', 'lasts', 'compete', 'conveniently', 'gained', 'reliable', 'authenticity', 'decidedly', 'expectation', 'charms', 'concentrate', 'lend', 'detract', 'incapable', 'dominated', 'undeniably', 'comprehend', 'wastes', 'encountered', 'rewarded', 'understatement', 'glimpses', 'benefits', 'defies', 'distract', 'void', 'celebrated', 'principals', 'disregard', 'faded', 'excruciatingly', 'comparable', 'versa', 'wherever', 'stretches', 'forgetting', 'ordeal', 'cracks', 'committing', 'pressed', 'nuances', 'nutshell', 'rips', 'climatic', 'pivotal', 'todays', 'readily', 'understandably', 'pitched', 'admitted', 'sufficient', 'increasing', 'anxious', 'popped', 'curiously', 'blockbusters', 'accompanying', 'transferred', 'oblivious', 'fairness', 'bless', 'ensue', 'inadvertently', 'listened', 'evolved', 'smoothly', 'seriousness', 'advances', 'motivated', 'greats', 'happenings', 'confirmed', 'overtones', 'explanations', 'assistance', 'switching', 'kidnaps']\n"
     ]
    }
   ],
   "source": [
    "sorted_by_tfidf = max_value.argsort() # sort by values vayc grum a indexneri hamarov\n",
    "top_n_words = [feature_names[i] for i in sorted_by_tfidf[:100]]\n",
    "print(\"Top 100 Most Important Words:\")\n",
    "print(top_n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f45d588c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Признаки с наименьшими значениями idf:\n",
      "['the' 'and' 'of' 'to' 'this' 'is' 'it' 'in' 'that' 'but' 'for' 'with'\n",
      " 'was' 'as' 'on' 'movie' 'not' 'have' 'one' 'be' 'film' 'are' 'you' 'all'\n",
      " 'at' 'an' 'by' 'so' 'from' 'like' 'who' 'if' 'they' 'there' 'out' 'his'\n",
      " 'just' 'about' 'or' 'he' 'has' 'what' 'some' 'can' 'good' 'when' 'more'\n",
      " 'time' 'up' 'very' 'even' 'only' 'see' 'no' 'my' 'would' 'really' 'well'\n",
      " 'which' 'story' 'had' 'me' 'much' 'than' 'their' 'were' 'other' 'get'\n",
      " 'do' 'been' 'don' 'most' 'also' 'into' 'first' 'how' 'great' 'her' 'will'\n",
      " 'made' 'because' 'people' 'make' 'way' 'bad' 'could' 'we' 'any' 'after'\n",
      " 'them' 'too' 'then' 'watch' 'think' 'movies' 'seen' 'acting' 'she'\n",
      " 'characters' 'its']\n"
     ]
    }
   ],
   "source": [
    "sorted_by_idf = np.argsort(vectt.idf_)\n",
    "print(\"Признаки с наименьшими значениями idf:\\n{}\".format(\n",
    " feature_names[sorted_by_idf[:100]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9845b0b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('worst', -1.2526641321757854),\n",
       " ('waste', -1.1385894471619764),\n",
       " ('awful', -0.9310996156458881),\n",
       " ('boring', -0.801194109017196),\n",
       " ('depp', -0.7986621836915596),\n",
       " ('disappointment', -0.7640757293850772),\n",
       " ('apocalyptic', 0.7477483813447101),\n",
       " ('excellent', 0.7395183856930316),\n",
       " ('horrible', -0.7095388989078968),\n",
       " ('fails', -0.7036363928890054),\n",
       " ('poorly', -0.6927296117359817),\n",
       " ('perfect', 0.6917187908680031),\n",
       " ('disappointing', -0.6912799951990269),\n",
       " ('save', -0.6899250067317967),\n",
       " ('laughable', -0.6821339426320414),\n",
       " ('great', 0.6794321479842673),\n",
       " ('terrible', -0.6711055916347928),\n",
       " ('definitive', 0.659863966765398),\n",
       " ('poor', -0.6499547850822516),\n",
       " ('avoid', -0.6366382377001891),\n",
       " ('cortese', -0.6300733338895728),\n",
       " ('wonderful', 0.6283286352359146),\n",
       " ('refreshing', 0.625469932130629),\n",
       " ('uninteresting', -0.6190807539298687),\n",
       " ('forgettable', -0.6171051766322657),\n",
       " ('reeling', 0.6131303593576428),\n",
       " ('notting', 0.6106285809393126),\n",
       " ('loved', 0.6062673628640093),\n",
       " ('ridiculous', -0.6050130127703717),\n",
       " ('dreadful', -0.595279750478861)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefficients = model.coef_[0]\n",
    "feature_names = vectt.get_feature_names_out()\n",
    "word_coefficient_pairs = list(zip(feature_names, coefficients))\n",
    "word_coefficient_pairs.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "word_coefficient_pairs[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0c4225",
   "metadata": {},
   "source": [
    "# Naive Bayes for multylabeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bea5ae16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8466"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(x_train, y_train)\n",
    "pred = nb_classifier.predict(x_test)\n",
    "metrics.accuracy_score(y_test, pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
